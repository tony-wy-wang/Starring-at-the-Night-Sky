{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.optim import lr_scheduler\n",
    "import copy\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pprint, pickle\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import DatasetFolder\n",
    "from PIL import Image\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import skimage.measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_round_pixel(data):\n",
    "    center_x= (10+120)\n",
    "    center_y=(9+120)\n",
    "    radius_outer = 120/(np.sqrt(2))\n",
    "    radius_inner=120 /2\n",
    "    x_len = 256\n",
    "    y_len = 256\n",
    "    out = np.zeros((x_len,y_len,30))\n",
    "    for i in range(x_len):\n",
    "        for j in range(y_len):\n",
    "            if ((center_x-i)**2 + (center_y-j)**2<=radius_outer**2):\n",
    "                out[i,j,:] = data[i,j,:]\n",
    "    #print(out.shape)\n",
    "    #print(out[int(center_x-radius_outer):int(center_x+radius_outer),int(center_y-radius_outer):int(center_y+radius_outer),:].shape)\n",
    "    out1 = out[int(center_x-radius_outer):int(center_x+radius_outer),int(center_y-radius_outer):int(center_y+radius_outer),:]\n",
    "    return out1\n",
    "def color_converter(data,transform,frames):\n",
    "    '''\n",
    "    converts the data from grey to rgb\n",
    "    input: 224,224,30\n",
    "    output: 30,3,224,224\n",
    "    '''\n",
    "    out = []\n",
    "    for i in range(frames):\n",
    "        grey = np.array(data[:,:,i], dtype = np.uint8)\n",
    "        grey = cv2.adaptiveThreshold(grey, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 3, 0)\n",
    "        colorimg = cv2.cvtColor(grey,cv2.COLOR_GRAY2RGB)\n",
    "        colorimg = np.asarray(colorimg)\n",
    "        colorimg = torchvision.transforms.ToPILImage(mode=None)(colorimg)\n",
    "        if transform is not None:\n",
    "            colorimg = transform(colorimg)\n",
    "        out.append(colorimg) #out is 30,224,224,3\n",
    "    out = torch.stack(out,dim=0)\n",
    "\n",
    "    \n",
    "    return out\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "i:166 (* 8)\n",
    "o:82  (*16)\n",
    "p:1330\n",
    "u:83  (*16)\n",
    "\"\"\" \n",
    "#backtorgb = cv2.cvtColor(gray,cv2.COLOR_GRAY2RGB)\n",
    "def default_loader(path):\n",
    "    data1 = np.load(path)\n",
    "    data1 = np.log(data1)\n",
    "    #data1 = data1//256\n",
    "    return data1\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, txt,loader=default_loader, frames = 30, transform=None, target_transform=None):\n",
    "        fh = open(txt, 'r')\n",
    "        imgs = []\n",
    "        for line in fh:\n",
    "            line = line.strip('\\n')\n",
    "            line = line.rstrip()\n",
    "            words = line.split()\n",
    "            if str(words[1]) == \"i\":\n",
    "                label = 0\n",
    "            elif str(words[1]) == \"o\":\n",
    "                label = 1\n",
    "            elif str(words[1]) == \"u\":\n",
    "                label = 2\n",
    "            else:\n",
    "                label = 3\n",
    "           \n",
    "            imgs.append((words[0],str(label)))\n",
    "            \n",
    "        self.frames = frames\n",
    "        self.imgs = imgs\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.loader = loader\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        fn, label = self.imgs[index]\n",
    "        img = self.loader(fn)\n",
    "        img = skimage.measure.block_reduce(img, (4,4,1), np.max)\n",
    "#         print(img.shape)\n",
    "        img = get_round_pixel(img)\n",
    "        img = color_converter(img,self.transform,self.frames)\n",
    "    \n",
    "#         img = img[256+128:768-128,256+128:768-128,:]#crop the cernter part of the image\n",
    "#         if self.transform is not None:\n",
    "#             img = self.transform(img)\n",
    "        \n",
    "#         img = torch.from_numpy(img)\n",
    "        label = list(map(int, label))\n",
    "        label = np.asarray(label)\n",
    "        label = label.astype(int)\n",
    "        label = torch.from_numpy(label)\n",
    "        #label= torch.LongTensor(label)    \n",
    "        return img,label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize([224,224]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([ \n",
    "        transforms.Resize([224,224]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize([224,224]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "valset = MyDataset(txt='E:\\\\TMT_SITE_RAID\\\\TMT_DATA_2\\\\T2-Armazones\\\\valset\\\\list.txt',transform=data_transforms['val'])\n",
    "trainset = MyDataset(txt='E:\\\\TMT_SITE_RAID\\\\TMT_DATA_2\\\\T2-Armazones\\\\Test_data\\\\list1.txt',transform=data_transforms['train'])\n",
    "testset = MyDataset(txt='E:\\\\TMT_SITE_RAID\\\\TMT_DATA_2\\\\T2-Armazones\\\\Testset\\\\list.txt',  transform=data_transforms['test'])\n",
    "train_size = 0.70\n",
    "val_size = 0.30\n",
    "num_train = len(trainset)\n",
    "indices = list(range(num_train))\n",
    "random.shuffle(indices)\n",
    "num_val = len(valset)\n",
    "valid_idx = list(range(num_val))\n",
    "num_test = len(testset)\n",
    "test_idx = list(range(num_test))\n",
    "split = int(np.floor(train_size * num_train))\n",
    "split_val = int(np.floor(val_size * num_train))\n",
    "\n",
    "# train_idx, valid_idx, test_idx = indices[:split], indices[split:split_val+split], indices[split_val+split:]\n",
    "# train_idx, valid_idx = indices[:split], indices[split:]\n",
    "\n",
    "image_datasets={'train': MyDataset(txt='E:\\\\TMT_SITE_RAID\\\\TMT_DATA_2\\\\T2-Armazones\\\\Test_Data\\\\list1.txt',  transform=data_transforms['train']),\n",
    "                'val': MyDataset(txt='E:\\\\TMT_SITE_RAID\\\\TMT_DATA_2\\\\T2-Armazones\\\\valset\\\\list.txt',  transform=data_transforms['val']),\n",
    "                'test': MyDataset(txt='E:\\\\TMT_SITE_RAID\\\\TMT_DATA_2\\\\T2-Armazones\\\\Testset\\\\list.txt',  transform=data_transforms['test']),\n",
    "                }\n",
    "train_sampler = torch.utils.data.SubsetRandomSampler(indices)\n",
    "val_sampler = torch.utils.data.SubsetRandomSampler(valid_idx)\n",
    "test_sampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "\n",
    "dataloaders = {\n",
    "        'train': torch.utils.data.DataLoader(trainset, batch_size=4,sampler = train_sampler,\n",
    "                                          shuffle=False, num_workers=0),\n",
    "        'val': torch.utils.data.DataLoader(valset, batch_size=4,sampler = val_sampler,\n",
    "                                          shuffle=False, num_workers=0),\n",
    "        'test': torch.utils.data.DataLoader(testset, batch_size=1,sampler = test_sampler,\n",
    "                                          shuffle=False, num_workers=0),\n",
    "                }\n",
    "dataset_sizes = {\n",
    "        'train':num_train,\n",
    "        'val': num_val,\n",
    "        'test': num_test\n",
    "\t\t\t\t    }\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 30, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 GPU!\n"
     ]
    }
   ],
   "source": [
    "# 2D CNN encoder using ResNet-152 pretrained\n",
    "class ResCNNEncoder(nn.Module):\n",
    "    def __init__(self, fc_hidden1=512, fc_hidden2=512, drop_p=0.3, CNN_embed_dim=300):\n",
    "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
    "        super(ResCNNEncoder, self).__init__()\n",
    "\n",
    "        self.fc_hidden1, self.fc_hidden2 = fc_hidden1, fc_hidden2\n",
    "        self.drop_p = drop_p\n",
    "\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1]      # delete the last fc layer.\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.fc1 = nn.Linear(resnet.fc.in_features, fc_hidden1)\n",
    "        self.bn1 = nn.BatchNorm1d(fc_hidden1, momentum=0.01)\n",
    "        self.fc2 = nn.Linear(fc_hidden1, fc_hidden2)\n",
    "        self.bn2 = nn.BatchNorm1d(fc_hidden2, momentum=0.01)\n",
    "        self.fc3 = nn.Linear(fc_hidden2, CNN_embed_dim)\n",
    "        \n",
    "    def forward(self, x_3d):\n",
    "        cnn_embed_seq = []\n",
    "        for t in range(x_3d.size(1)):\n",
    "            # ResNet CNN\n",
    "            with torch.no_grad():\n",
    "                x = self.resnet(x_3d[:, t, :, :, :])  # ResNet\n",
    "                x = x.view(x.size(0), -1)             # flatten output of conv\n",
    "\n",
    "            # FC layers\n",
    "            x = self.bn1(self.fc1(x))\n",
    "            x = F.relu(x)\n",
    "            x = self.bn2(self.fc2(x))\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.drop_p, training=self.training)\n",
    "            x = self.fc3(x)\n",
    "\n",
    "            cnn_embed_seq.append(x)\n",
    "\n",
    "        # swap time and sample dim such that (sample dim, time dim, CNN latent dim)\n",
    "        cnn_embed_seq = torch.stack(cnn_embed_seq, dim=0).transpose_(0, 1)\n",
    "        # cnn_embed_seq: shape=(batch, time_step, input_size)\n",
    "\n",
    "        return cnn_embed_seq\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, CNN_embed_dim=300, h_RNN_layers=3, h_RNN=256, h_FC_dim=128, drop_p=0.3, num_classes=4):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        self.RNN_input_size = CNN_embed_dim\n",
    "        self.h_RNN_layers = h_RNN_layers   # RNN hidden layers\n",
    "        self.h_RNN = h_RNN                 # RNN hidden nodes\n",
    "        self.h_FC_dim = h_FC_dim\n",
    "        self.drop_p = drop_p\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.LSTM = nn.LSTM(\n",
    "            input_size=self.RNN_input_size,\n",
    "            hidden_size=self.h_RNN,        \n",
    "            num_layers=h_RNN_layers,       \n",
    "            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(self.h_RNN, self.h_FC_dim)\n",
    "        self.fc2 = nn.Linear(self.h_FC_dim, self.num_classes)\n",
    "\n",
    "    def forward(self, x_RNN):\n",
    "        \n",
    "        self.LSTM.flatten_parameters()\n",
    "        RNN_out, (h_n, h_c) = self.LSTM(x_RNN, None)  \n",
    "        \"\"\" h_n shape (n_layers, batch, hidden_size), h_c shape (n_layers, batch, hidden_size) \"\"\" \n",
    "        \"\"\" None represents zero initial hidden state. RNN_out has shape=(batch, time_step, output_size) \"\"\"\n",
    "\n",
    "        # FC layers\n",
    "        x = self.fc1(RNN_out[:, -1, :])   # choose RNN_out at the last time step\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.drop_p, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "## ---------------------- end of CRNN module ---------------------- ##\n",
    "## ---------------------- start of training module ---------------------- ##\n",
    "\n",
    "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    # set model as training mode\n",
    "    cnn_encoder, rnn_decoder = model\n",
    "    cnn_encoder.train()\n",
    "    rnn_decoder.train()\n",
    "\n",
    "    losses = []\n",
    "    scores = []\n",
    "    N_count = 0   # counting total trained sample in one epoch\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        # distribute data to device\n",
    "        X, y = X.to(device), y.to(device).view(-1, )\n",
    "        y = y.long()\n",
    "\n",
    "        N_count += X.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = rnn_decoder(cnn_encoder(X))   # output has dim = (batch, number of classes)\n",
    "\n",
    "        loss = F.cross_entropy(output, y)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # to compute accuracy\n",
    "        y_pred = torch.max(output, 1)[1]  # y_pred != output\n",
    "        step_score = accuracy_score(y.cpu().data.squeeze().numpy(), y_pred.cpu().data.squeeze().numpy())\n",
    "        scores.append(step_score)         # computed on CPU\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # show information\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accu: {:.2f}%'.format(\n",
    "                epoch + 1, N_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item(), 100 * step_score))\n",
    "\n",
    "    return losses, scores\n",
    "## ---------------------- end of training module ---------------------- ##\n",
    "## ---------------------- start of validation module ---------------------- ##\n",
    "\n",
    "def validation(model, device, optimizer, test_loader):\n",
    "    # set model as testing mode\n",
    "    cnn_encoder, rnn_decoder = model\n",
    "    cnn_encoder.eval()\n",
    "    rnn_decoder.eval()\n",
    "    save_model_path = 'E:\\\\Starring-at-the-Night-Sky\\\\'\n",
    "\n",
    "    test_loss = 0\n",
    "    all_y = []\n",
    "    all_y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            # distribute data to device\n",
    "            X, y = X.to(device), y.to(device).view(-1, )\n",
    "            y = y.long()\n",
    "\n",
    "            output = rnn_decoder(cnn_encoder(X))\n",
    "\n",
    "            loss = F.cross_entropy(output, y, reduction='sum')\n",
    "            test_loss += loss.item()                 # sum up batch loss\n",
    "            y_pred = output.max(1, keepdim=True)[1]  # (y_pred != output) get the index of the max log-probability\n",
    "\n",
    "            # collect all y and y_pred in all batches\n",
    "            all_y.extend(y)\n",
    "            all_y_pred.extend(y_pred)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # compute accuracy\n",
    "    all_y = torch.stack(all_y, dim=0)\n",
    "    all_y_pred = torch.stack(all_y_pred, dim=0)\n",
    "    test_score = accuracy_score(all_y.cpu().data.squeeze().numpy(), all_y_pred.cpu().data.squeeze().numpy())\n",
    "\n",
    "    # show information\n",
    "    print('\\nTest set ({:d} samples): Average loss: {:.4f}, Accuracy: {:.2f}%\\n'.format(len(all_y), test_loss, 100* test_score))\n",
    "\n",
    "    # save Pytorch models of best record\n",
    "    torch.save(cnn_encoder.state_dict(), os.path.join(save_model_path, 'cnn_encoder_epoch{}.pth'.format(epoch + 1)))  # save spatial_encoder\n",
    "    torch.save(rnn_decoder.state_dict(), os.path.join(save_model_path, 'rnn_decoder_epoch{}.pth'.format(epoch + 1)))  # save motion_encoder\n",
    "    torch.save(optimizer.state_dict(), os.path.join(save_model_path, 'optimizer_epoch{}.pth'.format(epoch + 1)))      # save optimizer\n",
    "    print(\"Epoch {} model saved!\".format(epoch + 1))\n",
    "\n",
    "    return test_loss, test_score\n",
    "## ---------------------- end of validation module ---------------------- ##\n",
    "# Detect devices\n",
    "use_cuda = torch.cuda.is_available()                   # check if GPU exists\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")   # use CPU or GPU\n",
    "# EncoderCNN architecture\n",
    "CNN_fc_hidden1, CNN_fc_hidden2 = 1024, 768\n",
    "CNN_embed_dim = 512   # latent dim extracted by 2D CNN\n",
    "res_size = 224        # ResNet image size\n",
    "dropout_p = 0.0       # dropout probability\n",
    "# DecoderRNN architecture\n",
    "RNN_hidden_layers = 3\n",
    "RNN_hidden_nodes = 512\n",
    "RNN_FC_dim = 256\n",
    "# training parameters\n",
    "k = 4             # number of target category\n",
    "epochs = 100        # training epochs\n",
    "batch_size = 4 \n",
    "learning_rate = 1e-3\n",
    "log_interval = 10   # interval for displaying training info\n",
    "train_loader = dataloaders['train']\n",
    "valid_loader = dataloaders['val']\n",
    "# Create model\n",
    "cnn_encoder = ResCNNEncoder(fc_hidden1=CNN_fc_hidden1, fc_hidden2=CNN_fc_hidden2, drop_p=dropout_p, CNN_embed_dim=CNN_embed_dim).to(device)\n",
    "rnn_decoder = DecoderRNN(CNN_embed_dim=CNN_embed_dim, h_RNN_layers=RNN_hidden_layers, h_RNN=RNN_hidden_nodes, \n",
    "                         h_FC_dim=RNN_FC_dim, drop_p=dropout_p, num_classes=k).to(device)\n",
    "# Parallelize model to multiple GPUs\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    cnn_encoder = nn.DataParallel(cnn_encoder)\n",
    "    rnn_decoder = nn.DataParallel(rnn_decoder)\n",
    "\n",
    "    # Combine all EncoderCNN + DecoderRNN parameters\n",
    "    crnn_params = list(cnn_encoder.module.fc1.parameters()) + list(cnn_encoder.module.bn1.parameters()) + \\\n",
    "                  list(cnn_encoder.module.fc2.parameters()) + list(cnn_encoder.module.bn2.parameters()) + \\\n",
    "                  list(cnn_encoder.module.fc3.parameters()) + list(rnn_decoder.parameters())\n",
    "\n",
    "elif torch.cuda.device_count() == 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPU!\")\n",
    "    # Combine all EncoderCNN + DecoderRNN parameters\n",
    "    crnn_params = list(cnn_encoder.fc1.parameters()) + list(cnn_encoder.bn1.parameters()) + \\\n",
    "                  list(cnn_encoder.fc2.parameters()) + list(cnn_encoder.bn2.parameters()) + \\\n",
    "                  list(cnn_encoder.fc3.parameters()) + list(rnn_decoder.parameters())\n",
    "optimizer = torch.optim.Adam(crnn_params, lr=learning_rate)\n",
    "# record training process\n",
    "epoch_train_losses = []\n",
    "epoch_train_scores = []\n",
    "epoch_test_losses = []\n",
    "epoch_test_scores = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [40/4870 (1%)]\tLoss: 1.404392, Accu: 25.00%\n",
      "Train Epoch: 1 [80/4870 (2%)]\tLoss: 1.364099, Accu: 50.00%\n",
      "Train Epoch: 1 [120/4870 (2%)]\tLoss: 1.449009, Accu: 25.00%\n",
      "Train Epoch: 1 [160/4870 (3%)]\tLoss: 1.400172, Accu: 25.00%\n",
      "Train Epoch: 1 [200/4870 (4%)]\tLoss: 1.374553, Accu: 0.00%\n",
      "Train Epoch: 1 [240/4870 (5%)]\tLoss: 1.389832, Accu: 0.00%\n",
      "Train Epoch: 1 [280/4870 (6%)]\tLoss: 1.398194, Accu: 25.00%\n",
      "Train Epoch: 1 [320/4870 (7%)]\tLoss: 1.335523, Accu: 50.00%\n",
      "Train Epoch: 1 [360/4870 (7%)]\tLoss: 1.429649, Accu: 25.00%\n",
      "Train Epoch: 1 [400/4870 (8%)]\tLoss: 1.384765, Accu: 50.00%\n",
      "Train Epoch: 1 [440/4870 (9%)]\tLoss: 1.373457, Accu: 25.00%\n",
      "Train Epoch: 1 [480/4870 (10%)]\tLoss: 1.405240, Accu: 25.00%\n",
      "Train Epoch: 1 [520/4870 (11%)]\tLoss: 1.391598, Accu: 0.00%\n",
      "Train Epoch: 1 [560/4870 (11%)]\tLoss: 1.364906, Accu: 25.00%\n",
      "Train Epoch: 1 [600/4870 (12%)]\tLoss: 1.391363, Accu: 0.00%\n",
      "Train Epoch: 1 [640/4870 (13%)]\tLoss: 1.387389, Accu: 25.00%\n",
      "Train Epoch: 1 [680/4870 (14%)]\tLoss: 1.371229, Accu: 0.00%\n",
      "Train Epoch: 1 [720/4870 (15%)]\tLoss: 1.407665, Accu: 25.00%\n",
      "Train Epoch: 1 [760/4870 (16%)]\tLoss: 1.386677, Accu: 25.00%\n",
      "Train Epoch: 1 [800/4870 (16%)]\tLoss: 1.407162, Accu: 0.00%\n",
      "Train Epoch: 1 [840/4870 (17%)]\tLoss: 1.390657, Accu: 0.00%\n",
      "Train Epoch: 1 [880/4870 (18%)]\tLoss: 1.403726, Accu: 0.00%\n",
      "Train Epoch: 1 [920/4870 (19%)]\tLoss: 1.405490, Accu: 0.00%\n",
      "Train Epoch: 1 [960/4870 (20%)]\tLoss: 1.388578, Accu: 0.00%\n",
      "Train Epoch: 1 [1000/4870 (21%)]\tLoss: 1.402519, Accu: 0.00%\n",
      "Train Epoch: 1 [1040/4870 (21%)]\tLoss: 1.389711, Accu: 0.00%\n",
      "Train Epoch: 1 [1080/4870 (22%)]\tLoss: 1.377132, Accu: 50.00%\n",
      "Train Epoch: 1 [1120/4870 (23%)]\tLoss: 1.402770, Accu: 0.00%\n",
      "Train Epoch: 1 [1160/4870 (24%)]\tLoss: 1.384257, Accu: 25.00%\n",
      "Train Epoch: 1 [1200/4870 (25%)]\tLoss: 1.375832, Accu: 50.00%\n",
      "Train Epoch: 1 [1240/4870 (25%)]\tLoss: 1.393623, Accu: 25.00%\n",
      "Train Epoch: 1 [1280/4870 (26%)]\tLoss: 1.376909, Accu: 25.00%\n",
      "Train Epoch: 1 [1320/4870 (27%)]\tLoss: 1.380338, Accu: 0.00%\n",
      "Train Epoch: 1 [1360/4870 (28%)]\tLoss: 1.369261, Accu: 50.00%\n",
      "Train Epoch: 1 [1400/4870 (29%)]\tLoss: 1.406438, Accu: 0.00%\n",
      "Train Epoch: 1 [1440/4870 (30%)]\tLoss: 1.388835, Accu: 25.00%\n",
      "Train Epoch: 1 [1480/4870 (30%)]\tLoss: 1.367730, Accu: 50.00%\n",
      "Train Epoch: 1 [1520/4870 (31%)]\tLoss: 1.381483, Accu: 25.00%\n",
      "Train Epoch: 1 [1560/4870 (32%)]\tLoss: 1.355350, Accu: 75.00%\n",
      "Train Epoch: 1 [1600/4870 (33%)]\tLoss: 1.371868, Accu: 50.00%\n",
      "Train Epoch: 1 [1640/4870 (34%)]\tLoss: 1.364288, Accu: 50.00%\n",
      "Train Epoch: 1 [1680/4870 (34%)]\tLoss: 1.381904, Accu: 25.00%\n",
      "Train Epoch: 1 [1720/4870 (35%)]\tLoss: 1.401178, Accu: 25.00%\n",
      "Train Epoch: 1 [1760/4870 (36%)]\tLoss: 1.362361, Accu: 50.00%\n",
      "Train Epoch: 1 [1800/4870 (37%)]\tLoss: 1.411853, Accu: 25.00%\n",
      "Train Epoch: 1 [1840/4870 (38%)]\tLoss: 1.386099, Accu: 0.00%\n",
      "Train Epoch: 1 [1880/4870 (39%)]\tLoss: 1.362228, Accu: 75.00%\n",
      "Train Epoch: 1 [1920/4870 (39%)]\tLoss: 1.498217, Accu: 25.00%\n",
      "Train Epoch: 1 [1960/4870 (40%)]\tLoss: 1.391016, Accu: 0.00%\n",
      "Train Epoch: 1 [2000/4870 (41%)]\tLoss: 1.340450, Accu: 50.00%\n",
      "Train Epoch: 1 [2040/4870 (42%)]\tLoss: 1.407137, Accu: 25.00%\n",
      "Train Epoch: 1 [2080/4870 (43%)]\tLoss: 1.390401, Accu: 50.00%\n",
      "Train Epoch: 1 [2120/4870 (44%)]\tLoss: 1.354442, Accu: 50.00%\n",
      "Train Epoch: 1 [2160/4870 (44%)]\tLoss: 1.385178, Accu: 25.00%\n",
      "Train Epoch: 1 [2200/4870 (45%)]\tLoss: 1.385939, Accu: 0.00%\n",
      "Train Epoch: 1 [2240/4870 (46%)]\tLoss: 1.359738, Accu: 50.00%\n",
      "Train Epoch: 1 [2280/4870 (47%)]\tLoss: 1.426265, Accu: 0.00%\n",
      "Train Epoch: 1 [2320/4870 (48%)]\tLoss: 1.366825, Accu: 50.00%\n",
      "Train Epoch: 1 [2360/4870 (48%)]\tLoss: 1.340772, Accu: 25.00%\n",
      "Train Epoch: 1 [2400/4870 (49%)]\tLoss: 1.421298, Accu: 0.00%\n",
      "Train Epoch: 1 [2440/4870 (50%)]\tLoss: 1.395711, Accu: 0.00%\n",
      "Train Epoch: 1 [2480/4870 (51%)]\tLoss: 1.386828, Accu: 50.00%\n",
      "Train Epoch: 1 [2520/4870 (52%)]\tLoss: 1.130298, Accu: 50.00%\n",
      "Train Epoch: 1 [2560/4870 (53%)]\tLoss: 2.081198, Accu: 0.00%\n",
      "Train Epoch: 1 [2600/4870 (53%)]\tLoss: 1.352694, Accu: 50.00%\n",
      "Train Epoch: 1 [2640/4870 (54%)]\tLoss: 1.352388, Accu: 50.00%\n",
      "Train Epoch: 1 [2680/4870 (55%)]\tLoss: 1.329540, Accu: 0.00%\n",
      "Train Epoch: 1 [2720/4870 (56%)]\tLoss: 1.215842, Accu: 50.00%\n",
      "Train Epoch: 1 [2760/4870 (57%)]\tLoss: 1.188935, Accu: 75.00%\n",
      "Train Epoch: 1 [2800/4870 (57%)]\tLoss: 1.646866, Accu: 25.00%\n",
      "Train Epoch: 1 [2840/4870 (58%)]\tLoss: 1.354508, Accu: 25.00%\n",
      "Train Epoch: 1 [2880/4870 (59%)]\tLoss: 1.420591, Accu: 0.00%\n",
      "Train Epoch: 1 [2920/4870 (60%)]\tLoss: 1.297501, Accu: 25.00%\n",
      "Train Epoch: 1 [2960/4870 (61%)]\tLoss: 1.329056, Accu: 25.00%\n",
      "Train Epoch: 1 [3000/4870 (62%)]\tLoss: 1.354406, Accu: 25.00%\n",
      "Train Epoch: 1 [3040/4870 (62%)]\tLoss: 1.269539, Accu: 50.00%\n",
      "Train Epoch: 1 [3080/4870 (63%)]\tLoss: 1.357052, Accu: 25.00%\n",
      "Train Epoch: 1 [3120/4870 (64%)]\tLoss: 1.152368, Accu: 50.00%\n",
      "Train Epoch: 1 [3160/4870 (65%)]\tLoss: 1.577973, Accu: 50.00%\n",
      "Train Epoch: 1 [3200/4870 (66%)]\tLoss: 1.417382, Accu: 25.00%\n",
      "Train Epoch: 1 [3240/4870 (67%)]\tLoss: 1.449505, Accu: 0.00%\n",
      "Train Epoch: 1 [3280/4870 (67%)]\tLoss: 1.376063, Accu: 25.00%\n",
      "Train Epoch: 1 [3320/4870 (68%)]\tLoss: 1.345837, Accu: 25.00%\n",
      "Train Epoch: 1 [3360/4870 (69%)]\tLoss: 1.312985, Accu: 25.00%\n",
      "Train Epoch: 1 [3400/4870 (70%)]\tLoss: 1.355318, Accu: 25.00%\n",
      "Train Epoch: 1 [3440/4870 (71%)]\tLoss: 1.442179, Accu: 0.00%\n",
      "Train Epoch: 1 [3480/4870 (71%)]\tLoss: 1.368839, Accu: 25.00%\n",
      "Train Epoch: 1 [3520/4870 (72%)]\tLoss: 1.247365, Accu: 75.00%\n",
      "Train Epoch: 1 [3560/4870 (73%)]\tLoss: 1.328657, Accu: 50.00%\n",
      "Train Epoch: 1 [3600/4870 (74%)]\tLoss: 1.503850, Accu: 25.00%\n",
      "Train Epoch: 1 [3640/4870 (75%)]\tLoss: 1.296843, Accu: 50.00%\n",
      "Train Epoch: 1 [3680/4870 (76%)]\tLoss: 1.347629, Accu: 25.00%\n",
      "Train Epoch: 1 [3720/4870 (76%)]\tLoss: 1.252600, Accu: 50.00%\n",
      "Train Epoch: 1 [3760/4870 (77%)]\tLoss: 1.312416, Accu: 50.00%\n",
      "Train Epoch: 1 [3800/4870 (78%)]\tLoss: 1.555234, Accu: 25.00%\n",
      "Train Epoch: 1 [3840/4870 (79%)]\tLoss: 1.669455, Accu: 25.00%\n",
      "Train Epoch: 1 [3880/4870 (80%)]\tLoss: 1.265509, Accu: 25.00%\n",
      "Train Epoch: 1 [3920/4870 (80%)]\tLoss: 1.275877, Accu: 50.00%\n",
      "Train Epoch: 1 [3960/4870 (81%)]\tLoss: 1.404965, Accu: 0.00%\n",
      "Train Epoch: 1 [4000/4870 (82%)]\tLoss: 1.366266, Accu: 50.00%\n",
      "Train Epoch: 1 [4040/4870 (83%)]\tLoss: 1.467006, Accu: 0.00%\n",
      "Train Epoch: 1 [4080/4870 (84%)]\tLoss: 1.370102, Accu: 25.00%\n",
      "Train Epoch: 1 [4120/4870 (85%)]\tLoss: 1.374742, Accu: 25.00%\n",
      "Train Epoch: 1 [4160/4870 (85%)]\tLoss: 1.433693, Accu: 0.00%\n",
      "Train Epoch: 1 [4200/4870 (86%)]\tLoss: 1.392761, Accu: 0.00%\n",
      "Train Epoch: 1 [4240/4870 (87%)]\tLoss: 1.290194, Accu: 75.00%\n",
      "Train Epoch: 1 [4280/4870 (88%)]\tLoss: 1.416864, Accu: 25.00%\n",
      "Train Epoch: 1 [4320/4870 (89%)]\tLoss: 1.406269, Accu: 0.00%\n",
      "Train Epoch: 1 [4360/4870 (89%)]\tLoss: 1.529796, Accu: 0.00%\n",
      "Train Epoch: 1 [4400/4870 (90%)]\tLoss: 1.273214, Accu: 50.00%\n",
      "Train Epoch: 1 [4440/4870 (91%)]\tLoss: 1.392643, Accu: 25.00%\n",
      "Train Epoch: 1 [4480/4870 (92%)]\tLoss: 1.463041, Accu: 0.00%\n",
      "Train Epoch: 1 [4520/4870 (93%)]\tLoss: 1.434856, Accu: 25.00%\n",
      "Train Epoch: 1 [4560/4870 (94%)]\tLoss: 1.281102, Accu: 50.00%\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "for epoch in range(epochs):\n",
    "    # train, test model\n",
    "    train_losses, train_scores = train(log_interval, [cnn_encoder, rnn_decoder], device, train_loader, optimizer, epoch)\n",
    "    epoch_test_loss, epoch_test_score = validation([cnn_encoder, rnn_decoder], device, optimizer, valid_loader)\n",
    "\n",
    "    # save results\n",
    "    epoch_train_losses.append(train_losses)\n",
    "    epoch_train_scores.append(train_scores)\n",
    "    epoch_test_losses.append(epoch_test_loss)\n",
    "    epoch_test_scores.append(epoch_test_score)\n",
    "\n",
    "    # save all train test results\n",
    "    A = np.array(epoch_train_losses)\n",
    "    B = np.array(epoch_train_scores)\n",
    "    C = np.array(epoch_test_losses)\n",
    "    D = np.array(epoch_test_scores)\n",
    "    np.save('./CRNN_epoch_training_losses.npy', A)\n",
    "    np.save('./CRNN_epoch_training_scores.npy', B)\n",
    "    np.save('./CRNN_epoch_test_loss.npy', C)\n",
    "    np.save('./CRNN_epoch_test_score.npy', D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "float_img = np.random.random((4,4))\n",
    "im = np.array(float_img * 255, dtype = np.uint8)\n",
    "threshed = cv2.adaptiveThreshold(im, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 3, 0)\n",
    "colorimg = cv2.cvtColor(threshed,cv2.COLOR_GRAY2RGB)\n",
    "print(colorimg.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
