{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Incremental Principal Components Analysis.\"\"\"\n",
    "\n",
    "# Author: Kyle Kastner <kastnerkyle@gmail.com>\n",
    "#         Giorgio Patrini\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "\n",
    "from .base import _BasePCA\n",
    "from ..utils import check_array, gen_batches\n",
    "from ..utils.extmath import svd_flip, _incremental_mean_and_var\n",
    "\n",
    "\n",
    "class IncrementalKernelPCA(_BasePCA):\n",
    "    \"\"\"Incremental principal components analysis (IPCA).\n",
    "\n",
    "    Linear dimensionality reduction using Singular Value Decomposition of\n",
    "    the data, keeping only the most significant singular vectors to\n",
    "    project the data to a lower dimensional space. The input data is centered\n",
    "    but not scaled for each feature before applying the SVD.\n",
    "\n",
    "    Depending on the size of the input data, this algorithm can be much more\n",
    "    memory efficient than a PCA.\n",
    "\n",
    "    This algorithm has constant memory complexity, on the order\n",
    "    of ``batch_size``, enabling use of np.memmap files without loading the\n",
    "    entire file into memory.\n",
    "\n",
    "    The computational overhead of each SVD is\n",
    "    ``O(batch_size * n_features ** 2)``, but only 2 * batch_size samples\n",
    "    remain in memory at a time. There will be ``n_samples / batch_size`` SVD\n",
    "    computations to get the principal components, versus 1 large SVD of\n",
    "    complexity ``O(n_samples * n_features ** 2)`` for PCA.\n",
    "\n",
    "    Read more in the :ref:`User Guide <IncrementalPCA>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_components : int or None, (default=None)\n",
    "        Number of components to keep. If ``n_components `` is ``None``,\n",
    "        then ``n_components`` is set to ``min(n_samples, n_features)``.\n",
    "\n",
    "    whiten : bool, optional\n",
    "        When True (False by default) the ``components_`` vectors are divided\n",
    "        by ``n_samples`` times ``components_`` to ensure uncorrelated outputs\n",
    "        with unit component-wise variances.\n",
    "\n",
    "        Whitening will remove some information from the transformed signal\n",
    "        (the relative variance scales of the components) but can sometimes\n",
    "        improve the predictive accuracy of the downstream estimators by\n",
    "        making data respect some hard-wired assumptions.\n",
    "\n",
    "    copy : bool, (default=True)\n",
    "        If False, X will be overwritten. ``copy=False`` can be used to\n",
    "        save memory but is unsafe for general use.\n",
    "\n",
    "    batch_size : int or None, (default=None)\n",
    "        The number of samples to use for each batch. Only used when calling\n",
    "        ``fit``. If ``batch_size`` is ``None``, then ``batch_size``\n",
    "        is inferred from the data and set to ``5 * n_features``, to provide a\n",
    "        balance between approximation accuracy and memory consumption.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    components_ : array, shape (n_components, n_features)\n",
    "        Components with maximum variance.\n",
    "\n",
    "    explained_variance_ : array, shape (n_components,)\n",
    "        Variance explained by each of the selected components.\n",
    "\n",
    "    explained_variance_ratio_ : array, shape (n_components,)\n",
    "        Percentage of variance explained by each of the selected components.\n",
    "        If all components are stored, the sum of explained variances is equal\n",
    "        to 1.0.\n",
    "\n",
    "    singular_values_ : array, shape (n_components,)\n",
    "        The singular values corresponding to each of the selected components.\n",
    "        The singular values are equal to the 2-norms of the ``n_components``\n",
    "        variables in the lower-dimensional space.\n",
    "\n",
    "    mean_ : array, shape (n_features,)\n",
    "        Per-feature empirical mean, aggregate over calls to ``partial_fit``.\n",
    "\n",
    "    var_ : array, shape (n_features,)\n",
    "        Per-feature empirical variance, aggregate over calls to\n",
    "        ``partial_fit``.\n",
    "\n",
    "    noise_variance_ : float\n",
    "        The estimated noise covariance following the Probabilistic PCA model\n",
    "        from Tipping and Bishop 1999. See \"Pattern Recognition and\n",
    "        Machine Learning\" by C. Bishop, 12.2.1 p. 574 or\n",
    "        http://www.miketipping.com/papers/met-mppca.pdf.\n",
    "\n",
    "    n_components_ : int\n",
    "        The estimated number of components. Relevant when\n",
    "        ``n_components=None``.\n",
    "\n",
    "    n_samples_seen_ : int\n",
    "        The number of samples processed by the estimator. Will be reset on\n",
    "        new calls to fit, but increments across ``partial_fit`` calls.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.datasets import load_digits\n",
    "    >>> from sklearn.decomposition import IncrementalPCA\n",
    "    >>> X, _ = load_digits(return_X_y=True)\n",
    "    >>> transformer = IncrementalPCA(n_components=7, batch_size=200)\n",
    "    >>> # either partially fit on smaller batches of data\n",
    "    >>> transformer.partial_fit(X[:100, :])\n",
    "    IncrementalPCA(batch_size=200, copy=True, n_components=7, whiten=False)\n",
    "    >>> # or let the fit function itself divide the data into batches\n",
    "    >>> X_transformed = transformer.fit_transform(X)\n",
    "    >>> X_transformed.shape\n",
    "    (1797, 7)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Implements the incremental PCA model from:\n",
    "    *D. Ross, J. Lim, R. Lin, M. Yang, Incremental Learning for Robust Visual\n",
    "    Tracking, International Journal of Computer Vision, Volume 77, Issue 1-3,\n",
    "    pp. 125-141, May 2008.*\n",
    "    See https://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf\n",
    "\n",
    "    This model is an extension of the Sequential Karhunen-Loeve Transform from:\n",
    "    *A. Levy and M. Lindenbaum, Sequential Karhunen-Loeve Basis Extraction and\n",
    "    its Application to Images, IEEE Transactions on Image Processing, Volume 9,\n",
    "    Number 8, pp. 1371-1374, August 2000.*\n",
    "    See https://www.cs.technion.ac.il/~mic/doc/skl-ip.pdf\n",
    "\n",
    "    We have specifically abstained from an optimization used by authors of both\n",
    "    papers, a QR decomposition used in specific situations to reduce the\n",
    "    algorithmic complexity of the SVD. The source for this technique is\n",
    "    *Matrix Computations, Third Edition, G. Holub and C. Van Loan, Chapter 5,\n",
    "    section 5.4.4, pp 252-253.*. This technique has been omitted because it is\n",
    "    advantageous only when decomposing a matrix with ``n_samples`` (rows)\n",
    "    >= 5/3 * ``n_features`` (columns), and hurts the readability of the\n",
    "    implemented algorithm. This would be a good opportunity for future\n",
    "    optimization, if it is deemed necessary.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    D. Ross, J. Lim, R. Lin, M. Yang. Incremental Learning for Robust Visual\n",
    "    Tracking, International Journal of Computer Vision, Volume 77,\n",
    "    Issue 1-3, pp. 125-141, May 2008.\n",
    "\n",
    "    G. Golub and C. Van Loan. Matrix Computations, Third Edition, Chapter 5,\n",
    "    Section 5.4.4, pp. 252-253.\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    PCA\n",
    "    KernelPCA\n",
    "    SparsePCA\n",
    "    TruncatedSVD\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_components=None, whiten=False, copy=True,\n",
    "                 batch_size=None, m0, mmax=None, kernel=kernel_error, adjust=False,\n",
    "                     nystrom=False, maxiter=500, n):\n",
    "        self.n_components = n_components\n",
    "        self.whiten = whiten\n",
    "        self.copy = copy\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the model with X, using minibatches of size batch_size.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "\n",
    "        y : Ignored\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns the instance itself.\n",
    "        \"\"\"\n",
    "        self.components_ = None\n",
    "        self.n_samples_seen_ = 0\n",
    "        self.mean_ = .0\n",
    "        self.var_ = .0\n",
    "        self.singular_values_ = None\n",
    "        self.explained_variance_ = None\n",
    "        self.explained_variance_ratio_ = None\n",
    "        self.singular_values_ = None\n",
    "        self.noise_variance_ = None\n",
    "\n",
    "        X = check_array(X, copy=self.copy, dtype=[np.float64, np.float32])\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        if self.batch_size is None:\n",
    "            self.batch_size_ = 5 * n_features\n",
    "        else:\n",
    "            self.batch_size_ = self.batch_size\n",
    "\n",
    "        for batch in gen_batches(n_samples, self.batch_size_,\n",
    "                                 min_batch_size=self.n_components or 0):\n",
    "            self.partial_fit(X[batch], check_input=False)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def partial_fit(self, X, y=None, check_input=True):\n",
    "        \"\"\"Incremental fit with X. All of X is processed as a single batch.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        check_input : bool\n",
    "            Run check_array on X.\n",
    "\n",
    "        y : Ignored\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns the instance itself.\n",
    "        \"\"\"\n",
    "        if check_input:\n",
    "            X = check_array(X, copy=self.copy, dtype=[np.float64, np.float32])\n",
    "        n_samples, n_features = X.shape\n",
    "        if not hasattr(self, 'components_'):\n",
    "            self.components_ = None\n",
    "\n",
    "        if self.n_components is None:\n",
    "            if self.components_ is None:\n",
    "                self.n_components_ = min(n_samples, n_features)\n",
    "            else:\n",
    "                self.n_components_ = self.components_.shape[0]\n",
    "        elif not 1 <= self.n_components <= n_features:\n",
    "            raise ValueError(\"n_components=%r invalid for n_features=%d, need \"\n",
    "                             \"more rows than columns for IncrementalPCA \"\n",
    "                             \"processing\" % (self.n_components, n_features))\n",
    "        elif not self.n_components <= n_samples:\n",
    "            raise ValueError(\"n_components=%r must be less or equal to \"\n",
    "                             \"the batch number of samples \"\n",
    "                             \"%d.\" % (self.n_components, n_samples))\n",
    "        else:\n",
    "            self.n_components_ = self.n_components\n",
    "\n",
    "        if (self.components_ is not None) and (self.components_.shape[0] !=\n",
    "                                               self.n_components_):\n",
    "            raise ValueError(\"Number of input features has changed from %i \"\n",
    "                             \"to %i between calls to partial_fit! Try \"\n",
    "                             \"setting n_components to a fixed value.\" %\n",
    "                             (self.components_.shape[0], self.n_components_))\n",
    "\n",
    "        # This is the first partial_fit\n",
    "        if not hasattr(self, 'n_samples_seen_'):\n",
    "            self.n_samples_seen_ = 0\n",
    "            self.mean_ = .0\n",
    "            self.var_ = .0\n",
    "\n",
    "        # Update stats - they are 0 if this is the fisrt step\n",
    "        col_mean, col_var, n_total_samples = \\\n",
    "            _incremental_mean_and_var(\n",
    "                X, last_mean=self.mean_, last_variance=self.var_,\n",
    "                last_sample_count=np.repeat(self.n_samples_seen_, X.shape[1]))\n",
    "        n_total_samples = n_total_samples[0]\n",
    "\n",
    "        # Whitening\n",
    "        if self.n_samples_seen_ == 0:\n",
    "            # If it is the first step, simply whiten X\n",
    "            X -= col_mean\n",
    "        else:\n",
    "            col_batch_mean = np.mean(X, axis=0)\n",
    "            X -= col_batch_mean\n",
    "            # Build matrix of combined previous basis and new data\n",
    "            mean_correction = \\\n",
    "                np.sqrt((self.n_samples_seen_ * n_samples) /\n",
    "                        n_total_samples) * (self.mean_ - col_batch_mean)\n",
    "            X = np.vstack((self.singular_values_.reshape((-1, 1)) *\n",
    "                          self.components_, X, mean_correction))\n",
    "\n",
    "        U, S, V = linalg.svd(X, full_matrices=False)\n",
    "        U, V = svd_flip(U, V, u_based_decision=False)\n",
    "        explained_variance = S ** 2 / (n_total_samples - 1)\n",
    "        explained_variance_ratio = S ** 2 / np.sum(col_var * n_total_samples)\n",
    "\n",
    "        self.n_samples_seen_ = n_total_samples\n",
    "        self.components_ = V[:self.n_components_]\n",
    "        self.singular_values_ = S[:self.n_components_]\n",
    "        self.mean_ = col_mean\n",
    "        self.var_ = col_var\n",
    "        self.explained_variance_ = explained_variance[:self.n_components_]\n",
    "        self.explained_variance_ratio_ = \\\n",
    "            explained_variance_ratio[:self.n_components_]\n",
    "        if self.n_components_ < n_features:\n",
    "            self.noise_variance_ = \\\n",
    "                explained_variance[self.n_components_:].mean()\n",
    "        else:\n",
    "            self.noise_variance_ = 0.\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = 'E:\\\\TMT_DATA_2\\\\T2-Armazones'\n",
    "txt = r'E:\\\\TMT_DATA_2\\\\T2-Armazones\\\\usability_T2-Armazones_200510_200802_COMBINED .txt'\n",
    "#def get_data_array(root_dir,txt):\n",
    "b = read_txt(txt,image_folder)\n",
    "b.remove(b[0])\n",
    "b.remove(b[-1])\n",
    "txtlist = b\n",
    "str0 =  ''.join(b[0][0:3])\n",
    "dir0 = image_folder + \"\\\\\" + str0\n",
    "print(dir0)\n",
    "hour =  ''.join(b[0][3])\n",
    "n = len(txtlist)\n",
    "batch_n = 50\n",
    "iter_n = n//batch_n + 1\n",
    "transformer = IncrementalPCA(n_components=7, batch_size=batch_n)\n",
    "C1_transformed = np.zeros(1)\n",
    "C1_transformed = C1_transformed.tolist()\n",
    "C2_transformed = np.zeros(1)\n",
    "C2_transformed = C2_transformed.tolist()\n",
    "R1_transformed = np.zeros(1)\n",
    "R1_transformed = R1_transformed.tolist()\n",
    "R2_transformed = np.zeros(1)\n",
    "R2_transformed = R2_transformed.tolist()\n",
    "label_out = np.zeros(n)\n",
    "data1C_new = np.zeros([batch_n,np.shape(Loc_C)[0]])\n",
    "data2C_new = np.zeros([batch_n,np.shape(Loc_C)[0]])\n",
    "data1R_new = np.zeros([batch_n,np.shape(Loc_R)[0]])\n",
    "data2R_new = np.zeros([batch_n,np.shape(Loc_R)[0]])\n",
    "total = 0\n",
    "for i in range (iter_n):\n",
    "    print(\"training on \",i+1,\" of \",iter_n,\" batches\")\n",
    "    data1C_new = data1C_new - data1C_new\n",
    "    data2C_new = data2C_new - data2C_new\n",
    "    data1R_new = data1R_new - data1R_new\n",
    "    data2R_new = data2R_new - data2R_new\n",
    "    for j in range(batch_n):\n",
    "        str0 =  ''.join(txtlist[i*batch_n+j][0:3])\n",
    "        dir0 = image_folder + \"\\\\\" + str0\n",
    "        hour =  ''.join(txtlist[i*batch_n+j][3])\n",
    "        label = ''.join(txtlist[i*batch_n+j][4])\n",
    "        label = convert_label(label)\n",
    "        data1_sum, data2_sum = get_hourly_mean(dir0,hour)\n",
    "        if np.sum(data1_sum) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            label_out[i*batch_n+j] = label\n",
    "            data1C_new[j,:] = Get_Data(data1_sum,Loc_C).T\n",
    "            total = total + 1\n",
    "            data2C_new[j,:] = Get_Data(data2_sum,Loc_C).T\n",
    "            data1R_new[j,:] = Get_Data(data1_sum,Loc_R).T\n",
    "            data2R_new[j,:] = Get_Data(data2_sum,Loc_R).T\n",
    "    \n",
    "    C1_transformed.extend(transformer.fit_transform(data1C_new))\n",
    "    R1_transformed.extend(transformer.fit_transform(data1R_new))\n",
    "    C2_transformed.extend(transformer.fit_transform(data2C_new))\n",
    "    R2_transformed.extend(transformer.fit_transform(data2R_new))\n",
    "    \n",
    "    \n",
    "C1_transformed = C1_transformed[1:]\n",
    "R1_transformed = R1_transformed[1:]\n",
    "R1_transformed = R1_transformed[1:]\n",
    "R2_transformed = R2_transformed[1:]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5, 6] [1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = [[1,2,3],[4,5,6]]\n",
    "b = np.random.permutation(100)\n",
    "print(a[1],a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from astropy.visualization import astropy_mpl_style\n",
    "from astropy.io import fits\n",
    "import astropy\n",
    "plt.style.use(astropy_mpl_style)\n",
    "from astropy.utils.data import get_pkg_data_filename\n",
    "import numpy as np\n",
    "#import ogr\n",
    "#import shapely.wkt\n",
    "#import shapely.geometry\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import json\n",
    "import csv\n",
    "from collections import Counter\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "import os, glob\n",
    "from sklearn import decomposition\n",
    "from sklearn import cluster\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from scipy import spatial\n",
    "from io import StringIO\n",
    "import importlib\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from astropy.utils.data import get_pkg_data_filename\n",
    "from astropy.io import fits\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import scipy.misc\n",
    "import math\n",
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_normalization(data): #normalizes data to a gaussian\n",
    "    new_data = preprocessing.normalize(np.log(data))\n",
    "    return new_data\n",
    "\n",
    "def Get_Round_Pixel_Locations(data,radius,center_x,center_y): \n",
    "    #gets the coordinates of pixels in the round arrea. Outputs a n*2 array\n",
    "    len_y = np.shape(data)[0]\n",
    "    len_x = np.shape(data)[1]\n",
    "    out = [[None,None]] #stores the coordinate of the pixels\n",
    "    for j in range(0,len_y-1):\n",
    "        for i in range(0,len_x-1):\n",
    "            if ((i-center_x)**2 + (j-center_y)**2) < radius**2:\n",
    "                out.append([i,j])\n",
    "    out = np.delete(out, 0, 0)  # delete first row of out\n",
    "    return out #Returns the Loc: location of the pixels\n",
    "\n",
    "def Get_Ring_Pixel_Locations(data,radius_inner,radius_outer,center_x,center_y): \n",
    "    #gets the coordinates of pixels in the round arrea. Outputs a n*2 array\n",
    "    len_y = np.shape(data)[0]\n",
    "    len_x = np.shape(data)[1]\n",
    "    out = [[None,None]] #stores the coordinate of the pixels\n",
    "    for j in range(0,len_y-1):\n",
    "        for i in range(0,len_x-1):\n",
    "            if ((((i-center_x)**2 + (j-center_y)**2) <= radius_outer**2) & (((i-center_x)**2 + (j-center_y)**2) >= radius_inner**2) ):\n",
    "                out.append([i,j])\n",
    "    out = np.delete(out, 0, 0)  # delete first row of out\n",
    "    return out #Returns the Loc: location of the pixels\n",
    "\n",
    "def Get_Data(data, Loc): \n",
    "    #returns the converted pixels using the coordinates\n",
    "    n = np.shape(Loc)[0]\n",
    "    out = np.zeros([np.shape(Loc)[0],1])\n",
    "    for i in range(0,n-1):\n",
    "        out[i] = data[Loc[i,0],Loc[i,1]]\n",
    "    return out\n",
    "\n",
    "def Reconstruct_Data(data_Vec,Loc,x_len,y_len):\n",
    "    n = np.shape(Loc)[0]\n",
    "    out = np.zeros([x_len,y_len])\n",
    "    for i in range(0,n-1):\n",
    "        out[Loc[i,0],Loc[i,1]] = data_Vec[i]\n",
    "    return out\n",
    "\n",
    "def read_txt(txt,filename): #reads the txt file, converts the txt file to list\n",
    "    a = []\n",
    "    with open(txt, 'r') as f:\n",
    "        while True:\n",
    "            line = f.readline() \n",
    "            b = line.split()\n",
    "            a.append(b)\n",
    "            if not line:\n",
    "                return a\n",
    "                break     \n",
    "                \n",
    "def convert_label(label): #tokenize the labels\n",
    "    if label == 'n':  \n",
    "        out = 0\n",
    "    elif label == 'm':\n",
    "        out = 1\n",
    "    elif label == 'o':\n",
    "        out = 2\n",
    "    elif label == 'p':\n",
    "        out = 3\n",
    "    elif label == 'i':\n",
    "        out = 4\n",
    "    elif label == 'u':\n",
    "        out = 5\n",
    "    else:\n",
    "        out = 6\n",
    "    return out\n",
    "\n",
    "# def get_hourly_mean(rootDir,hour,Loc_C,Loc_R): #calculates the mean data hourly, outputs the blue and red band data\n",
    "#     list_dirs = os.walk(rootDir)\n",
    "#     a = ''\n",
    "#     cnt1 = 0\n",
    "#     cnt2 = 0\n",
    "#     data1C_sum = np.zeros([np.shape(Loc_C)[0]])\n",
    "#     data2C_sum = np.zeros([np.shape(Loc_C)[0]])\n",
    "#     data1R_sum = np.zeros([np.shape(Loc_R)[0]])\n",
    "#     data2R_sum = np.zeros([np.shape(Loc_R)[0]])\n",
    "#     for root, dirs, files in list_dirs: \n",
    "#         for d in files: \n",
    "#             path = os.path.join(root, d)\n",
    "#             str0 = path.split('\\\\')[-1]\n",
    "#             str1 = str0[3]\n",
    "#             str2 = str0[14]+str0[15]\n",
    "            \n",
    "#             if str1 == \"b\":\n",
    "#                 if str2 == hour:\n",
    "#                     cnt1 += 1\n",
    "#                     image_file = path\n",
    "#                     fits.info(image_file)\n",
    "#                     image_data = fits.getdata(image_file, ext=0)\n",
    "#                     image_data = data_normalization(np.log(image_data))\n",
    "#                     data_C1 = Get_Data(image_data,Loc_C)\n",
    "#                     #data_R1 = Get_Data(image_data,Loc_R)\n",
    "#                     data1C_sum = data1C_sum + data_C1\n",
    "#                     #data1R_sum = data1R_sum + data_R1\n",
    "                    \n",
    "#             elif str1 == \"r\":\n",
    "#                 if str2 == hour:\n",
    "#                     cnt2 += 1\n",
    "#                     image_file = path\n",
    "#                     #fits.info(image_file)\n",
    "#                     image_data = fits.getdata(image_file, ext=0)\n",
    "#                     image_data = data_normalization(np.log(image_data))\n",
    "#                     #data_C2 = Get_Data(image_data,Loc_C)\n",
    "#                     #data_R2 = Get_Data(image_data,Loc_R)\n",
    "#                     #data2C_sum = data2C_sum + data_C2\n",
    "#                     #data2R_sum = data2R_sum + data_R2\n",
    "#             #print(str)\n",
    "#     data1C_sum = data1C_sum/cnt1\n",
    "#     data1R_sum = data1R_sum/cnt1\n",
    "#     data2C_sum = data2C_sum/cnt2\n",
    "#     data2R_sum = data2R_sum/cnt2\n",
    "#     return data1C_sum, data1R_sum, data2C_sum,data2R_sum\n",
    "def get_hourly_mean(rootDir,hour): #calculates the mean data hourly, outputs the blue and red band data\n",
    "    list_dirs = os.walk(rootDir)\n",
    "    a = ''\n",
    "    cnt1 = 0\n",
    "    cnt2 = 0\n",
    "    data1 = np.zeros([1024,1024])\n",
    "    data2 = np.zeros([1024,1024])\n",
    "    for root, dirs, files in list_dirs: \n",
    "        for d in files: \n",
    "            path = os.path.join(root, d)\n",
    "            str0 = path.split('\\\\')[-1]\n",
    "            str1 = str0[3]\n",
    "            str2 = str0[14]+str0[15]\n",
    "            \n",
    "            if str1 == \"b\":\n",
    "                if str2 == hour:\n",
    "                    cnt1 += 1\n",
    "                    image_file = path\n",
    "                    #fits.info(image_file)\n",
    "                    \n",
    "                    image_data = fits.getdata(image_file, ext=0)\n",
    "                    #image_data = data_normalization(np.log(image_data))\n",
    "                    if image_data.shape[0] != 1024:\n",
    "                        cnt1 -=1\n",
    "                        continue\n",
    "                    else:\n",
    "                        data1 = data1 + image_data\n",
    "                    #print(np.max(data1))\n",
    "                    \n",
    "            elif str1 == \"r\":\n",
    "                if str2 == hour:\n",
    "                    cnt2 += 1\n",
    "                    image_file = path\n",
    "                    #fits.info(image_file)\n",
    "                    \n",
    "                    image_data = fits.getdata(image_file, ext=0)\n",
    "                    \n",
    "                    #image_data = data_normalization(np.log(image_data))\n",
    "                    if image_data.shape[0] != 1024:\n",
    "                        cnt2 -= 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        data2 = data2 + image_data\n",
    "            #print(str)\n",
    "    if cnt1 * cnt2 == 0:\n",
    "        data1 = np.zeros([1024,1024])\n",
    "        data2 = np.zeros([1024,1024])\n",
    "        return data1,data2\n",
    "    data1 = data1/cnt1\n",
    "    data2 = data2/cnt2\n",
    "    data1 = data_normalization(data1)\n",
    "    data2 = data_normalization(data2)\n",
    "    return data1, data2    \n",
    "\n",
    "def data_organizer(txtlist,Loc_C,Loc_R): #reshapes the data and restructure all data in the list. data: d*n label: n*1\n",
    "    num_data = len(txtlist)\n",
    "    #print(num_data)\n",
    "    data_bC = np.zeros([np.shape(Loc_C)[0],num_data])\n",
    "    #print(np.shape(data_bC))\n",
    "#     data_bR = np.zeros([np.shape(Loc_R)[0],num_data])\n",
    "#     data_rC = np.zeros([np.shape(Loc_C)[0],num_data])\n",
    "#     data_rR = np.zeros([np.shape(Loc_R)[0],num_data])\n",
    "    label_out = np.zeros(num_data)\n",
    "    for i in range (num_data):\n",
    "        str0 =  ''.join(txtlist[i][0:3])\n",
    "        dir0 = image_folder + \"\\\\\" + str0\n",
    "        hour =  ''.join(txtlist[i][3])\n",
    "        label = ''.join(txtlist[i][4])\n",
    "        label = convert_label(label)\n",
    "        label_out[i] = label\n",
    "        data1C_sum, data1R_sum, data2C_sum,data2R_sum = get_hourly_mean(dir0,hour,Loc_C,Loc_R)\n",
    "        data_bC[:,i] = data1C_sum\n",
    "        return data_bC,label_out\n",
    "#         data_bR[:,i] = data1R_sum\n",
    "#         data_rC[:,i] = data2C_sum\n",
    "#         data_rR[:,i] = data2R_sum\n",
    "#     return data_bC,data_bR,data_rC,data_rR,label_out\n",
    "#def get_next_data(txtlist,Loc_C,Loc_R,i):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a toy data\n",
    "#image_file = get_pkg_data_filename('tutorials/FITS-images/HorseHead.fits')\n",
    "image_file = r'C:\\USB backup\\Sem1, 2019\\ENGN8602\\sample_data\\to_b20061127ut005945s19380.fits.gz'\n",
    "#image_file = r'E:\\T1-Tolar.20061126.asca\\net\\data\\ASCA\\20061126\\to_b20061127ut000753s16260.fits.gz'\n",
    "print(image_file)\n",
    "fits.info(image_file)\n",
    "image_data = fits.getdata(image_file, ext=0)\n",
    "plt.figure()\n",
    "#plt.imshow(data_normalization(np.log(image_data)), cmap='gray')\n",
    "#plt.colorbar()\n",
    "image_data = data_normalization(np.log(image_data))\n",
    "r_out = 470\n",
    "r_in = 470/np.sqrt(2)\n",
    "x = 55+470\n",
    "y = 35+470\n",
    "#get locations\n",
    "Loc_C = Get_Round_Pixel_Locations(image_data,r_in,x,y)\n",
    "Loc_R = Get_Ring_Pixel_Locations(image_data,r_in,r_out,x,y)\n",
    "#get data\n",
    "#data_C = Get_Data(image_data,Loc_C)\n",
    "#data_R = Get_Data(image_data,Loc_R)\n",
    "\n",
    "print(np.shape(Loc_C)[0])\n",
    "print(np.shape(Loc_R)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = 'E:\\\\TMT_DATA_2\\\\T2-Armazones'\n",
    "txt = r'E:\\\\TMT_DATA_2\\\\T2-Armazones\\\\usability_T2-Armazones_200510_200802_COMBINED .txt'\n",
    "#def get_data_array(root_dir,txt):\n",
    "b = read_txt(txt,image_folder)\n",
    "b.remove(b[0])\n",
    "b.remove(b[-1])\n",
    "txtlist = b\n",
    "str0 =  ''.join(b[0][0:3])\n",
    "dir0 = image_folder + \"\\\\\" + str0\n",
    "print(dir0)\n",
    "hour =  ''.join(b[0][3])\n",
    "n = len(txtlist)\n",
    "batch_n = 50\n",
    "iter_n = n//batch_n + 1\n",
    "transformer = IncrementalPCA(n_components=7, batch_size=batch_n)\n",
    "C1_transformed = np.zeros(1)\n",
    "C1_transformed = C1_transformed.tolist()\n",
    "C2_transformed = np.zeros(1)\n",
    "C2_transformed = C2_transformed.tolist()\n",
    "R1_transformed = np.zeros(1)\n",
    "R1_transformed = R1_transformed.tolist()\n",
    "R2_transformed = np.zeros(1)\n",
    "R2_transformed = R2_transformed.tolist()\n",
    "label_out = np.zeros(n)\n",
    "data1C_new = np.zeros([batch_n,np.shape(Loc_C)[0]])\n",
    "data2C_new = np.zeros([batch_n,np.shape(Loc_C)[0]])\n",
    "data1R_new = np.zeros([batch_n,np.shape(Loc_R)[0]])\n",
    "data2R_new = np.zeros([batch_n,np.shape(Loc_R)[0]])\n",
    "total = 0\n",
    "for i in range (iter_n):\n",
    "    print(\"training on \",i+1,\" of \",iter_n,\" batches\")\n",
    "    data1C_new = data1C_new - data1C_new\n",
    "    data2C_new = data2C_new - data2C_new\n",
    "    data1R_new = data1R_new - data1R_new\n",
    "    data2R_new = data2R_new - data2R_new\n",
    "    for j in range(batch_n):\n",
    "        str0 =  ''.join(txtlist[i*batch_n+j][0:3])\n",
    "        dir0 = image_folder + \"\\\\\" + str0\n",
    "        hour =  ''.join(txtlist[i*batch_n+j][3])\n",
    "        label = ''.join(txtlist[i*batch_n+j][4])\n",
    "        label = convert_label(label)\n",
    "        data1_sum, data2_sum = get_hourly_mean(dir0,hour)\n",
    "        if np.sum(data1_sum) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            label_out[i*batch_n+j] = label\n",
    "            data1C_new[j,:] = Get_Data(data1_sum,Loc_C).T\n",
    "            total = total + 1\n",
    "            data2C_new[j,:] = Get_Data(data2_sum,Loc_C).T\n",
    "            data1R_new[j,:] = Get_Data(data1_sum,Loc_R).T\n",
    "            data2R_new[j,:] = Get_Data(data2_sum,Loc_R).T\n",
    "    \n",
    "    C1_transformed.extend(transformer.fit_transform(data1C_new))\n",
    "    R1_transformed.extend(transformer.fit_transform(data1R_new))\n",
    "    C2_transformed.extend(transformer.fit_transform(data2C_new))\n",
    "    R2_transformed.extend(transformer.fit_transform(data2R_new))\n",
    "    \n",
    "    \n",
    "C1_transformed = C1_transformed[1:]\n",
    "R1_transformed = R1_transformed[1:]\n",
    "R1_transformed = R1_transformed[1:]\n",
    "R2_transformed = R2_transformed[1:]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 8)\n",
      "(4, 8)\n",
      "(5, 8)\n",
      "(6, 8)\n",
      "(7, 8)\n"
     ]
    }
   ],
   "source": [
    "class Counter:\n",
    "    def __init__(self, low, high):\n",
    "        self.current = low\n",
    "        self.high = high\n",
    "\n",
    "    def fit(self): # Python 3: def __next__(self)\n",
    "        if self.current > self.high:\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            self.current += 1\n",
    "            return (self.current - 1, self.high)\n",
    "\n",
    "counter = Counter(3,8)\n",
    "for i in range(5):\n",
    "    print(counter.fit())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
